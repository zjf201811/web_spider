    1. 爬虫如何自动识别一个网站的正文标题而不需要配置一些参数？

谢邀，在这个问题的回答里看到了不少熟人啊神箭手作为一家技术立命的公司，作为CEO的我显然不能不来答一下这个问题。这个问题我分成三个层面来回答下一、神箭手对这个问题的处理先说说神箭手的情况吧，这个问题我们很久之前就开始研究了，一直期待能做到一个优秀的效果，就像友商@Jerry黄大仙 说的一样，新闻场景中的正文标题识别确实是相对好做的，可以直接hard coding一些特征。因为新闻标题在页面中出现的地方是很有特点的，比如在title标签中大概率出现，比如在正文上面大概率有h1,h2等标签，比如下面大概率有作者，发布时间等部分。显然可以看出，这个方案类似于特征工程师的工作，虽然看似原始，但事实上在大多数场景下是最快速的问题解决方案。目前在类似百度新闻搜索这样的采集场景是非常好用的，但是如果做全网爬虫依然面临两个问题：1.无法识别出来这个页面是不是文章页 2.有的非新闻的文章标题还是很不一样的。而且全靠人来做处理也存在一个天花板的问题，一个人能处理的信息是有限的，因此我们现在也逐步过渡到了深度学习的方案上去了，这个我在方案三具体来讲。二、谷歌内部的处理之前在谷歌工作时，center-piece组（不确定拼的对不对，老同事不要来拍砖）就坐我旁边。这个组专门分析页面中哪些重要的信息是我们关心的，哪些信息是我们不关心的（识别正文和标题）。大家可能以为谷歌的方案一定很高大上，然而事实并不是这样，谷歌内部对于Google Brain这样的高大上的解决方案的使用率并不高（可能是由于黑盒的原因），不过由于谷歌有大量的日志信息（像是搜索日志，还有chrome日志，不过这个高度敏感，一般团队申请不到）通过日志行为加上页面特征的做一些hard coding，再结合渲染js，这个大公司有钱做计算，可以对大量页面都做一次js渲染之后再处理。因此准确度会比我们自己来做hardcoding要好不少，当然我不是那个组的，具体方案我知道了估计也不能说，就瞎扯这么多吧。三、diffbot方案事实上diffbot算是这个领域做的最完善的一家公司了，神箭手也在参照的他们的方案在做。以前如果要使用神经网络来分析页面中标题是什么，正文是什么。大多采用的方案是将dom树特征和css特征手动提取出来，然后在扔到一个神经网络里去做BP训练，这样的好处是计算量小，坏处是依然需要特征工程，而且不够通用，经常会出现一些页面不符合要求的还得重做特征工程，跟方案一就没区别了（说不定还不如方案一）。根据diffbot的公开信息，他们是以图像识别为主算法结合NLP等其他技术来做的元素类型识别。当然，他们不会告诉大家他们完整的算法，不过这里我做一个推测，也是目前神箭手的思路，拿出来跟大家分享，也欢迎大家私信来讨论。首先对于图像方案，最重要的是渲染所有能渲染的部分，包括图片，CSS,AJAX等等，因此需要一个完整渲染，当然技术上很简单，通过无头浏览器就可以轻松实现。接着如何做呢，我们有如下选择：1.不做整体渲染，而是对每一个dom元素进行渲染（可以通过一些特征先简单做一下筛选），然后扔进cnn直接做分类，简单粗暴。问题有两个：首先dom元素很多，导致要渲染很多次，没必要。其次一个元素是否是标题不仅仅是由他自己的样子决定的，还由他周边的元素样子决定。2.做整体渲染，同时我们可以通过js获取到每个dom在整体图片中的位置（x,y,width,height）然后在整张图中截取类似（centorX,centorY,width*2,height*2）的这样一张图扔进CNN做分类，这样做解决了多次渲染和没有周边元素的问题，但是显然计算量可以说和1一样，是非常的大。3.做整体识别，首先先把整张图做一次分类，判断是文章页，列表页，商品页还是什么。然后针对每种页面直接扔进cnn，输出一个对应类型所包含的所有元素的矩形框（一种类型的页面，包含的元素数量是固定的），比如文章页，就输出向量 {Pt（有title的概率）,Xt,Yt,Wt,Ht,...} 根据向量里的元素和现有的dom元素进行一个位置对比，找出跟识别出来的位置最大重合的dom的元素，就认为是相应的元素。这个方案最大的问题，一方面训练分两步，导致所有样本首先要被分成不同类型，最终训练出来的model也有多个，导致实践中数据和算法很难处理，不是end to end。另一方面识别的时候也会比较慢。4.将整张图的分类问题和输出boundingbox放在一次计算中，参考YOLO算法，首先训练的时候，先将标题，评论这些元素单独渲染并单独训练一个分类model，然后在整体将图片进行划分成多个cell，让每一个cell输出一个非背景元素的概率，一个boundingbox和分类向量，同时前几位用一个softmax输出网页类型的分类，这样虽然在训练的过程中会比较麻烦，但是在实际识别的时确实一个标准的endtoend的方案，速度会快很多。这个方案最大的问题就是像是作者，发布时间这类太小了，导致如果分隔的cell太大，容易识别不出来。cell太小，运算量又大，而且训练的更慢。综上，单纯用图像算法，还是很难解决比如评论中的评论者头像这类问题，最终还是需要结合dom树以及文本特征做一些微观的处理，不过宏观上大块的分隔确实鲁棒性高了很多，也通用了很多。目前神箭手正在完善相关的算法，以及训练数据的处理。不过爬虫确实可以方便的解决掉大部分的训练数据问题。后续我们也会推出类似diffbot的解析API。当然现在这个最大的问题还是太慢了，完整渲染一下网页估计就得花掉10s+。对于爬虫大数据/人工智能相关方向感兴趣的朋友都可以关注我或者专栏：我：吴桐-神箭手CEO专栏：数据黑板-知乎专栏





    2. 如何解决爬虫过程中网页中数字解析为方块的问题？

谢邀 看这张截图可以看出来这个是Chrome查看源码时的表现，根据题主的提问，应该是在正常访问是是一个年月日时分秒的样子，那么根据这个表现就可以确认是字体反爬了，具体的做法就是将这个span设置成一个特殊字体，这个字体看样子是只对数字做了特殊处理。有很多网站，都是会用字体这个方式对一些重要的文本进行加密，比如显示天眼查的公司核准日期，汽车之家的论坛口碑,去哪儿机票手机版上的价格什么的。神箭手上大部分都有写好的爬虫，不想处理的可以直接移步我们官网 www.shenjian.io解决方案：参考@自称都会派 的回答，我完善一下。首先普及一下字体知识，比如ttf字体内部有一个Glyphs的index和charcode（一般是Unicode的字符集）的对应mapping数组，页面上直接爬取到的是charcode，显示字体是会通过这个charcode到mapping数组中查找到glyphs的index，再去查找这个字体的具体图像（矢量保存），然后绘制在屏幕上。也就是这里面有两个最重要的索引，一个是charcode，对应的你爬到的字符，一个是glyphs的index，对应的是真实显示出来的字符样子。由上，我们可以分成三种情况，题主这个情况应该是属于情况二：情况一、CharCode和Glyphs都不变这种情况太简单了，甚至不用管字体了，题主直接在页面上对于charcode找规律就行，不过一般来说不大可能，这样就白做字体加密了情况二、CharCode变化，但是Glyphs不变这种情况比较常见（主要是做反爬的时候比较好实现一些，比如用一个成熟字体的Glyphs，然后只动态生成CharCode和Glyphs的映射关系就行了），这种情况只需要解析字体文件中的mapping表，然后先预定义好不同Glyphs代表的具体字符，然后通过查mapping就可以转换出来了。具体题主的这个问题应该是对应这种情况。神箭手云上提供了parseTTF方法，专门用来解析字体的mapping表的。如果不用神箭手，也可以自己去找类似的库。我之前的专栏里有专门提到这种情况怎么处理，可以参考：爬虫与诡异的字体-反爬与反反爬的奇技淫巧情况三、CharCode和Glyph都变化这种情况最大的问题还是在Glyph的index也变化了，即使查出来Glyph的index，也不知道具体显示出来的样子是什么。这种情况显然很难处理，而且实际中我从来都没有遇到过。真的要是遇到了，提供几个思路可以参考：3.1有限字符集的情况，比如只要数字字体，那么把字体里面Glyph的data拿出来做一次md5，用这个md5作为字符的index3.2无限字符集（严格意义上没有无限字符，也就是很多字符，实在工作量太大的情况）但是显示字符长度短，比如公司名（公司名可能包含各种字符，但是一般长度不会超过20），建议使用直接用深度学习做一下图像分类，因为字体你是有的，你可以直接生成出来图片样本来学习，我之前专栏里也有文章可以参考：Discuz验证码识别（准备篇）-写给程序员的TensorFlow教程3.3很多字符的同时文本长度也很长。说真的，反爬能做到这个份上。我觉得真的可以跪拜下这个反爬工程师了，建议不要爬这个网站了。对于爬虫大数据/人工智能相关方向感兴趣的朋友都可以关注我或者专栏：我：吴桐-神箭手CEO专栏：数据黑板-知乎专栏





    3. 分布式云爬虫，未来的主流采集方式？

题主 你好，我是神箭手的CEO，首先非常高兴你能关注到我们的服务。关于更新的问题我需要解释一下，由于服务企业的数量的增加，我们早期的架构很难满足增长的用户的需求，所以我们决定在去年10月份左右对全套系统从底层进行重构，由于对整体工作量预估不足，导致整个重构工作拖了3个月左右，期间确实对老版本和老用户的维护显得不足，导致大家对我们的工作有一些抱怨，不过虽然我们的同事一边重构 一边修复bug已经非常辛苦，但我们依然还是要求所有的人在客户的问题上尽全力解决。--------------------------------------------------一本正经的分割线------------------------------------------------------------下面随便聊聊关于题主谈到的其他问题1.先谈谈开发这样一套系统的花费吧，首先开发这样的系统作为CEO来说肯定是要技术的，否则光找技术合伙人估计黄花菜就已经凉了，我自己之前是在Google工作，而且工作前也有过几次的创业经历，算是对开发一套新系统非常熟悉了，各种坑也算是踩的差不多。所以应该说，还是给团队节省了不少钱，算是资本效率比较高的；其次这样一套系统本身看似模块并不多，实际上大量的细节体验会耗费无数的时间，而这样一个无论从技术本身还是从用户体验角度都是非常有挑战的系统，比平常开发一套社交软件会复杂1~2个数量级，对开发人员的素质要求也会比普通的移动应用和网站高出不少，因此薪资方面就会有很大的压力，当然 如果能自己干，还是能省不少钱，不过需要涉及到前端，后端，分布式，用户体验。差不多最少需要5个左右的牛逼闪闪的全栈工程师做一年吧。2.关于市场方面，一年几万的收入本身肯定不算多的，知乎很多大牛估计是要笑话的，当然我们真实的收入有多少，我也不方便公布。本身网络爬虫市场很难去具体定义市场大小，因为直接需要爬虫的客户并不多，而自己独特业务结合爬虫的场景也不是特别容易标准化，因此单纯谈网络爬虫市场恐怕是要让楼主失望的，我们之所以从一开始就跟其他采集器看着气质迥异，画风清奇。就是因为我们是希望能做成一个分布式的开发平台，大大降低分布式应用的开发成本，服务器资源也可以即用即买，爬虫只是我们的第一站，当然也是很重要的一站。3.最后从竞争方面，我们还是希望能看到更多的同行，毕竟就算从整个大数据服务来看，在中国起步时间也不长，很多传统公司还没有意识到各个维度的数据对公司的重大意义。因此我们是希望能有更多的同行跟我们一起携手把市场做大。最后的最后，我们的新版即将上线！ 更多牛逼的功能已经在路上了， 欢迎杭州的同学们来加入我们，一起开启下个时代的数据服务。





    4. 类似新榜或传送门之类的微信爬虫具体是如何实现的？

要爬取微信公众号的文章，可以用这个现成的：微信文章[按公众号爬取]采集爬虫





    5. 制作一个数据采集的网页(软件)要多少钱？

你可以去神箭手云爬虫开发平台这个上面去看看，有很多现成的采集爬虫可以直接跑。





    6. 用Python写爬虫，用什么方式、框架比较好？

我开发了一个云端爬虫开发框架：神箭手，可以让开发者在云上使用Javascript编写和运行爬虫，欢迎大家来使用拍砖~





    7. PHP, Python, Node.js 哪个比较适合写爬虫？

JavaScript+神箭手云爬虫开发平台（http://shenjianshou.cn）。神箭手是我们团队开发的在线编写爬虫的云框架，只需要简单几行js就可以实现复杂的爬虫，编写执行都在云上进行，不需要配置开发环境。另外提供很多爬虫扩展功能，比如反反爬虫、JS渲染、数据发布和导出、图表控件等。欢迎大家都来试试，给我们多提意见~





    8. 怎样利用数据爬取和分析工具写出《黄焖鸡米饭是怎么火起来的》这样的文章？

去年开始研究做爬虫，搞了一套分布式的爬虫系统，主要目标是帮别人做数据采集。后来看到黄焖鸡米饭是怎么火起来的？ - 何明科的回答，进而关注了《数据冰山》，发现里面的大数据分析的文章都相当有意思，图表也一个比一个专业。我当时的表情大约是这样的：我的天哪，这么神奇吗？ 放下手机，操起键盘，正准备也搞上一篇 “大数据分析：郭德纲和女演员的相爱相杀之后，wuli涛涛是如何火起来的”。后来转念一想，这不是赤果果的抄袭了，妈妈是怎么教导我的。再说了，以我这样的实力，写了这样的文章，以后别人还怎么写呢？俗话说得好：授人以鱼不如授人以渔，独乐乐不如众乐乐。不如我们就以黄焖鸡米饭为例，给大家讲讲如何才能写出这样一篇图文并茂的分析文章来吧。先来一段硬广：本文所有代码，都需要运行在本人搭建的神箭手云爬虫框架上，打算完全自己写爬虫的同学，领会精神即可。数据来源分析首先需要黄焖鸡米饭门店的创建时间，来分析黄焖鸡米饭随时间的增长，其次需要门店的地域信息来分析不同地域黄焖鸡米饭的增长情况。分析大众点评的商户门店信息，可以在商户的贡献榜页面找到相关的信息，如下图：这里需要对数据作几个近似处理：仅选取商户名中包含"黄焖鸡米饭"的门店将商户的添加时间近似看作门店的创建时间大众点评无法查到已经关闭的商户，所以这里不考虑门店的关闭，仅选取现存的门店开始写爬虫上面分析了对数据的需求，下面就开始动手写爬虫爬取数据啦~ 熟悉爬虫的人都知道，一个爬虫的基本工作流程是：首先挑选一部分种子URL（也可以叫入口URL），并放入到待爬队列中从待爬队列中取出一个URL，下载内容并从中抽取信息，同时发现新URL，并加入到待爬队列中。重复此步骤，直至待爬队列为空。上面加粗了3个重点，种子URL、抽取信息和发现新URL。种子URL也可以叫入口URL，爬虫以这些URL为入口，以某种规则发现新的URL，最终爬遍所有想要的网页。为了爬取高效，我决定直接用大众点评的搜索，选择大众点评的搜索结果页作为入口URL，爬取结果页的所有商户并筛选后作为样本数据。大众点评的搜索也是分区域的，要把所有区域的搜索结果页都作为入口URL，形如http://www.dianping.com/search/keyword/{region_id}/0_%E9%BB%84%E7%84%96%E9%B8%A1%E7%B1%B3%E9%A5%AD
其中region_id从1到2323（很容易可以发现此范围内是中国的区域，数字再大就到国外了，如果多了或者少了请告诉我）。抽取信息从网页中抽取信息，最常用的是xpath，这里我们需要抽取商户id（防止重复），商户名称（过滤掉不含黄焖鸡米饭的），创建时间，区域名称，省份是没有的，需要根据区域名称得到。xpath可以结合Chrome的开发者工具来写，并通过xpath插件来验证，下面给出这几项数据的xpath：商户id，抽取的数据中包含其他商户id，需要进一步处理来得到id//div[contains(@class,'shop-review-wrap')]/div/h3/a/@href
商户名称//div[contains(@class,'shop-review-wrap')]/div/h3/a/text()
创建时间，需要进一步字符串处理后得到时间//div[contains(@class,'block raw-block')]/ul/li[1]/span
区域名称，同样文本需要处理//div[@class='breadcrumb']/b[1]/a/span/text()
发现新URLURL的发现规则不是必须配置的，但是配置之后，可以大大提高爬虫的速率。对于大众点评这样规整的列表页+详情页，配置好列表页的url规则和详情页的url规则，爬虫的目标就很明确，爬取速率杠杠的。一般这种规则用正则来表示，对于这里的爬虫，列表页规则为http://www.dianping.com/search/keyword/\\d+/0_.*
详情页规则为http://www.dianping.com/shop/\\d+/editmember
另外，大众点评限制了IP的访问频率，这里可以把降低爬取速率，或是使用代理。如果你使用的是神箭手云爬虫，则代码如下configs.enableProxy = true;
自己写爬虫的同学，请自行Google代理IP，此处不再赘述。用爬取的数据配置出图表折腾出了这么多代码，约摸着看到这的都是真爱了，那么赶紧看看我们的成果吧：总共爬取到14000多条数据，现在就以这些数据作为样本来分析（下面涉及到的图表的设置，都是在神箭手云平台上操作完成的）。1. 黄焖鸡米饭的整体增长像原文里的分析一样，以季度为单位，作出2012年至2016年，黄焖鸡米饭的门店数随时间的增长情况。先看下出来的图：上图中，柱形绘制的是各个季度门店的新增数，折线绘制的是截止到某个季度的总门店数。对于柱形图/折线图，首先设置X轴和Y轴。Y轴比较简单，就是门店数，值类型是value，一个Y轴就可以了，不需要第二个Y轴；X轴是按季度划分的，值类型是category，需要在爬取结果的create_time字段上作区间划分，这里展示的"新增"和"总店数"两个数据，它们的统计区间是不一样的，"新增"统计的是create_time落在某个季度的门店数，而"总店数"统计的是create_time在某个季度之前的门店数，所以这里的区间划分需要定义两个。X轴和Y轴定义好之后，开始定义数据。新增数展示为柱形图。X轴的字段选择create_time，在create_time上划分区间，这里通过简单的字符串比较就可以划分出季度区间，比如"2013-Q2"，定义它的最小值为"2013-04-01"，最大值为"2013-07-01"。Y轴只需要对划分出的区间作count操作就可以，所以配置Y轴字段为'*'，操作选择计数。总店数展示为折线图。总店数跟新增数唯一的区别就是区间划分，比如"2013-Q2"，只定义它的最大值为"2013-07-01"就好了。下图为部分设置界面。配置完成后保存，就可以查看生成的图表了。2. 分区域分析黄焖鸡米饭的增长逐年观察各个省份黄焖鸡米饭门店数的增长情况，时间维度体现在多张图上，省份的数据通过中国地图的着色深浅来表示。先看效果图：图表类型为中国地图，配置也比较简单。首先在create_time上添加过滤条件来配置年份，比如要配置2014年的地图，添加过滤条件，字段选择create_time，设置最大值为"2015-01-01"；配置区域字段为province_name，数据还是计数操作，选择字段为'*'，操作为count。这样就会筛选出2014年底黄焖鸡米饭的门店，并以省为单位，分别统计门店个数。最后设置图例，不同门店数用不同的颜色填充，就可以作出上面的系列图。大功告成，一碗热腾腾的黄焖鸡米饭就可以上桌了，如果想搞什么大盘鸡，小盘鸡，红烧肉,KTV啥的的，相信你都已经不在话下了吧。完整代码：samples/dianping.js at master · ShenJianShou/samples · GitHub





